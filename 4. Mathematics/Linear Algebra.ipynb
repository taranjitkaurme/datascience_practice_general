{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1331bccf-e5bf-4bd0-8d03-60cb2dcbff35",
   "metadata": {},
   "source": [
    "# Detailed Linear Algebra for Data Science\n",
    "\n",
    "## Introduction\n",
    "This document provides an in-depth look at the key concepts of linear algebra essential for data science. Each section includes definitions, explanations, and examples relevant to the application in data science.\n",
    "\n",
    "## Vectors and Spaces\n",
    "\n",
    "### Definition of Scalers and Vectors\n",
    "\n",
    "- **Scalars**:  These are quantities that have only magnitude (size or amount) but no direction. Think of them like simple numbers that tell you how much of something there is, but not where it's going. A good example is temperature. If it's 30 degrees Celsius, that number tells you how hot it is, but not in which direction the heat is going. Other examples include things like speed, mass, and time.\r\n",
    "- **Vectors**:s: On the other hand, vectors are quantities that have both magnitude and direction. They're like arrows pointing somewhere, with the length of the arrow showing how much there is of something and the direction of the arrow showing where it's going. For example, if you're driving a car, your velocity is a vector. It tells you not just how fast you're going (the speed) but also in which direction. Similarly, force is a vector since it's not enough to know how strong the force is; you also need to know in which direction it is applied.\r\n",
    "\r\n",
    "So, in short, scalars are just simple numbers indicating how much of something there is, while vectors are like arrows that tell you how much there is and in which direction it's going.\n",
    "\n",
    "- **Scalars vs. Vectors**: Scalars are single numbers, while vectors consist of multiple scalar val\n",
    "  ues.\n",
    "- **Vector Operations**:\n",
    "    - **Addition**: Adding corresponding elements of two vectors.\n",
    "    - **Subtraction**: Subtracting corresponding elements of two vectors.\n",
    "    - **Scalar Multiplication**: Multiplying each element of a vector by a scal\n",
    "\n",
    "### Vector Spaces\n",
    "\n",
    "Vector spaces, \"span\" and \"basis\" are two important concepts that help us understand how vectors work together in a space. Let's break them down in simple terms:\n",
    "- **Span and Basis**:\n",
    "    - **Span**: Set of all possible linear combinations of a set of vectors.The span of a set of vectors is all of the vectors you can make by combining those vectors. When you combine vectors, you're doing two things: scaling them (multiplying them by numbers) and adding them together. The span is like a recipe book that tells you all the different dishes (vectors) you can cook up using a specific set of ingredients (the original vectors). If the span of a set of vectors includes every possible vector in the space, then we say that these vectors span the entire space. For example, in a two-dimensional space (like a flat sheet of paper), if you have two non-parallel arrows (vectors), you can reach any point on the paper by stretching, shrinking, and combining these arrows. So, these two arrows span the two-dimensional space.\n",
    "\n",
    "      \n",
    "    - **Basis**: The minimum set of linearly independent vectors that span a vector space. A basis of a vector space is a set of vectors that are both linearly independent (none of the vectors can be made by combining the others) and span the entire space. You can think of a basis as the most \"efficient\" set of ingredients in your recipe book. With these ingredients (vectors), you can make every possible dish (vector in the space) without any leftovers or waste. In our two-dimensional space example, if you have two arrows that are not pointing in the same or opposite directions, they can serve as a basis. You can reach any point on the paper using combinations of these two arrows, and neither arrow is redundant (you can't create one arrow by using the other).\n",
    "    - In simpler terms, the span tells you what you can reach with your vectors, and the basis is the minimal, non-redundant set of vectors you need to reach everything in the span\n",
    "      \n",
    "- **Linear Independence**: A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. span.ers.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5c1f5b-3094-4813-8d3e-3abd85ad8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: [3 7]\n",
      "Subtraction: [ 1 -1]\n",
      "Scalar Multiplication: [4 6]\n"
     ]
    }
   ],
   "source": [
    "## Vectors and Spaces\n",
    "\n",
    "### Python Example: Vector Operations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define two vectors\n",
    "vector1 = np.array([2, 3])\n",
    "vector2 = np.array([1, 4])\n",
    "\n",
    "# Vector addition\n",
    "addition = vector1 + vector2\n",
    "\n",
    "# Vector subtraction\n",
    "subtraction = vector1 - vector2\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar_multiplication = 2 * vector1\n",
    "\n",
    "print(\"Addition:\", addition)\n",
    "print(\"Subtraction:\", subtraction)\n",
    "print(\"Scalar Multiplication:\", scalar_multiplication)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a053f-ae95-4841-bb20-0e97a49c4628",
   "metadata": {},
   "source": [
    "## Matrix Algebra\n",
    "\n",
    "### Matrices and Matrix Operations\n",
    "- **Matrices**: A matrix is a rectangular array of numbers, symbols, or expressions.\n",
    "- **Matrix Operations**:\n",
    "    - **Addition/Subtraction**: Element-wise addition or subtraction.\n",
    "    - **Scalar Multiplication**: Multiplying each element of the matrix by a scalar.\n",
    "    - **Matrix Multiplication**: Combining rows of the first matrix with columns of the second.\n",
    "\n",
    "### Types of Matrices\n",
    "- **Identity Matrix**: A square matrix with 1's along the diagonal and 0's elsewhere.\n",
    "- **Diagonal Matrix**: A matrix where the entries outside the diagonal are all zero.\n",
    "- **Symmetric Matrix**: A matrix that is equal to its transpose.\n",
    "\n",
    "### Matrix Inversion and Determinants\n",
    "- **Inverse of a Matrix**: A matrix that, when multiplied with the original matrix, yields the identity matrix.\n",
    "- **Determinant of a Matrix**: A scalar value that represents the volume scaling factor of the linear transformation described by the matrix.\n",
    "The determinant of a matrix, is a special number that can be calculated from the elements of a square matrix (a matrix with the same number of rows and columns). It has several important interpretations and uses:\n",
    "    \r\n",
    "Volume Scaling Factor: Imagine a shape being transformed in space, like stretching or squishing a cube into a different shape. The determinant tells you how much the volume of that shape changes due to the transformation. A determinant of 1 means the volume stays the same, greater than 1 means the volume increases, and less than 1 means it decreases.    \r\n",
    "\r\n",
    "Matrix Invertibility: The determinant can tell you whether a matrix has an inverse. If the determinant is 0, the matrix cannot be inverted, which in geometric terms means the transformation squashes the shape into a lower-dimensional space (like squishing a 3D shape into a flat 2D shape    ).\r\n",
    "\r\n",
    "Orientation Change: The sign of the determinant (positive or negative) indicates whether the transformation preserves the orientation of the shape. A positive determinant means the orientation is preserved, while a negative determinant means it is reversed (like turning a glove inside out).\r\n",
    "\r\n",
    "In summary, the determinant provides a single value that encapsulates important geometric properties of a matrix, particularly in relation to transformations it represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb242a1c-0fbc-4352-a983-6e8e25b8621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[ 6  8]\n",
      " [10 12]]\n",
      "Matrix Multiplication:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two matrices\n",
    "matrix1 = np.array([[1, 2], [3, 4]])\n",
    "matrix2 = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix addition\n",
    "addition = matrix1 + matrix2\n",
    "\n",
    "# Matrix multiplication\n",
    "multiplication = np.dot(matrix1, matrix2)\n",
    "\n",
    "print(\"Matrix Addition:\\n\", addition)\n",
    "print(\"Matrix Multiplication:\\n\", multiplication)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e98092-e960-4c02-8fb4-c8948b5d1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Matrix (3x3):\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Diagonal Matrix:\n",
      " [[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n",
      "\n",
      "Symmetric Matrix:\n",
      " [[1 2 3]\n",
      " [2 4 5]\n",
      " [3 5 6]]\n",
      "\n",
      "Inverse of a Matrix:\n",
      " [[ 0.6 -0.7]\n",
      " [-0.2  0.4]]\n",
      "\n",
      "Determinant of a Matrix: 10.000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Identity Matrix (3x3)\n",
    "identity_matrix = np.eye(3)\n",
    "print(\"Identity Matrix (3x3):\\n\", identity_matrix)\n",
    "\n",
    "# 2. Diagonal Matrix\n",
    "diagonal_matrix = np.diag([1, 2, 3])\n",
    "print(\"\\nDiagonal Matrix:\\n\", diagonal_matrix)\n",
    "\n",
    "# 3. Symmetric Matrix\n",
    "symmetric_matrix = np.array([[1, 2, 3],\n",
    "                             [2, 4, 5],\n",
    "                             [3, 5, 6]])\n",
    "print(\"\\nSymmetric Matrix:\\n\", symmetric_matrix)\n",
    "\n",
    "# Matrix for inversion and determinant (must be square and non-singular)\n",
    "matrix_for_inversion_and_determinant = np.array([[4, 7],\n",
    "                                                 [2, 6]])\n",
    "\n",
    "# 4. Inverse of a Matrix\n",
    "inverse_matrix = np.linalg.inv(matrix_for_inversion_and_determinant)\n",
    "print(\"\\nInverse of a Matrix:\\n\", inverse_matrix)\n",
    "\n",
    "# 5. Determinant of a Matrix\n",
    "determinant = np.linalg.det(matrix_for_inversion_and_determinant)\n",
    "print(\"\\nDeterminant of a Matrix:\", determinant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcca168-a70b-48cd-be70-e10641ed169e",
   "metadata": {},
   "source": [
    "Calculating the determinant of a matrix is a fundamental operation in linear algebra. The method varies depending on the size of the matrix. Here's a general guide for calculating the determinant:\n",
    "\n",
    "**2x2 Matrix**\n",
    "For a 2x2 matrix, the determinant is calculated as follows:\n",
    "\n",
    "Given a matrix:\n",
    "\n",
    "( a  b\n",
    "\n",
    "  c  d )\n",
    "\n",
    "The determinant is:\n",
    "det = ad−bc\n",
    "\n",
    "**3x3 Matrix**\n",
    "\n",
    "For a 3x3 matrix, the calculation is slightly more complex. The determinant is calculated by expanding along a row or a column (here, I'll use the first row for simplicity):\n",
    "\n",
    "Given a matrix:\n",
    "\n",
    "( a b c\n",
    "\n",
    " d e f\n",
    " \n",
    " g h i )\n",
    "\n",
    "det=a(ei−fh)−b(di−fg)+c(dh−eg)\n",
    "\n",
    "This method involves finding the determinants of three 2x2 matrices and combining them.\n",
    "\n",
    "The determinant of the given matrix \r",
    "\r\n",
    "1\r\n",
    " \r\n",
    "4\r",
    " 3\r\n",
    "\n",
    " \n",
    "\n",
    " \r\n",
    "\r",
    "1\r\n",
    "is \r\n",
    "−\r\n",
    "2.0\r\n",
    "−2.0.\r\n",
    "\r\n",
    "Here's what this determinant tells us:\r\n",
    "\r\n",
    "Volume Change: The absolute value of the determinant (2.0) indicates how much the area (since it's a 2x2 matrix) is scaled when a geometric transformation represented by this matrix is applied. In this case, any area would be scaled by a factor of 2.\r\n",
    "\r\n",
    "Invertibility: Since the determinant is not zero, this matrix is invertible. It means there exists another matrix which, when multiplied with this one, will give the identity matrix.\r\n",
    "\r\n",
    "Orientation: The negative sign of the determinant (-2.0) is significant. It indicates that the transformation would reverse the orientation of the space. For instance, if this matrix were used to transform a shape in 2D space, the shape would be flipped over, like turning a piece of paper over to its other side.\r\n",
    "\r\n",
    "So, the determinant gives us valuable information about how the matrix transforms space, including scaling, invertibility, and orientation changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccde68d-18e6-4ed4-84c0-e33fab3ee7c5",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "### Introduction to Eigenvalues and Eigenvectors\n",
    "- **Definition**: \n",
    "Eigenvalues and eigenvectors are concepts from linear algebra that have broad applications in fields like engineering, physics, and data analysis. Let's break them down into layman's terms:\n",
    "\n",
    "- **Eigenvectors**:  Non-zero vectors that change at most by a scalar factor when that linear transformation is applied.Imagine you have a transformation machine that changes the shape or direction of objects. You put in a vector (think of it as an arrow pointing in a certain direction), and the machine transforms it. Now, most vectors will come out of this machine pointing in a different direction. However, there might be some special vectors (eigenvectors) that come out still pointing in their original direction, though their length might have changed. These special vectors are called eigenvectors of the transformation.\n",
    "\n",
    "- **Eigenvalues**:Scalars that measure the factor by which the eigenvector is scaled during a linear transformation.The eigenvalue is a number associated with each eigenvector. It tells you how much the length of the eigenvector changes when it goes through the transformation. If the eigenvalue is 2, for example, the eigenvector becomes twice as long. If it's 0.5, the eigenvector becomes half as long. If the eigenvalue is 1, the eigenvector doesn't change in length at all.\n",
    "\n",
    "In simpler terms, in the world of transformations, eigenvectors are the vectors that manage to keep their direction, and eigenvalues tell us how their length is altered. They are important because they reveal the fundamental properties of the transformation, such as which directions are stretched or shrunk and by how much.\n",
    "\n",
    "### Calculating Eigenvalues and Eigenvectors\n",
    "- **Characteristic Polynomial**: A polynomial which is derived from the determinant of a matrix and is used to find eigenvalues.\n",
    "- **Applications in Dimensionality Reduction (PCA)**:\n",
    "    - Principal Component Analysis (PCA) uses eigenvalues and eigenvectors to reduce the dimensionality of data sets, improving the interpretability while minimizing information loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1733ec1-0964-42bb-b72d-ac0abe8414fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a matrix\n",
    "matrix = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35cae2-a497-45b7-940e-4ab2f664d1a4",
   "metadata": {},
   "source": [
    "\n",
    "## Singular Value Decomposition\n",
    "\n",
    "### Understanding SVD\n",
    "- **Concept and Mathematical Formulation**: \n",
    "    - Decomposition of a matrix into three other matrices. It provides a way of diagonalizing a matrix.\n",
    "    - Equation: `A = UΣV*` where A is the original matrix, U and V are orthogonal matrices, and Σ is a diagonal matrix of singular values.\n",
    "\n",
    "### Applications of SVD in Data Science\n",
    "- **Data Compression**: Reducing the size of data by eliminating redundant features.\n",
    "- **Noise Reduction**: Improving the quality of data by removing noise.\n",
    "\n",
    "## Applications in Data Science\n",
    "\n",
    "### Linear Algebra in Machine Learning\n",
    "- **Feature Spaces**: Using vector spaces to represent features in machine learning models.\n",
    "- **Linear Regression, PCA, and Other Algorithms**: \n",
    "    - Linear regression involves matrix operations for predicting outcomes.\n",
    "    - PCA for dimensionality reduction.\n",
    "    - Other algorithms where linear algebra is pivotal.\n",
    "\n",
    "### Network Analysis\n",
    "- **Adjacency Matrices**: Representing graphs and networks using matrices.\n",
    "- **Graph Theory Concepts**: Understanding the structure and properties of networks.\n",
    "\n",
    "## Conclusion\n",
    "- Recap of the importance of linear algebra in data science.\n",
    "- Discussion on how these concepts are integral to understanding data and building predictive models.\n",
    "\n",
    "## References\n",
    "- [Linear Algebra and Its Applications, David C. Lay]\n",
    "- [Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares, Stephen Boyd and Lieven Vandenberghe]\n",
    "- [Online resources and courses for further study.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0646a-74ab-42aa-b8ca-a566f333b051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
