{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2d24eb-ebf9-4672-9c9b-04de4002d13e",
   "metadata": {},
   "source": [
    "# Linear Algebra for Data Science\n",
    "\n",
    "## Introduction\n",
    "This document provides an in-depth look at the key concepts of linear algebra essential for data science. Each section includes definitions, explanations, and examples relevant to the application in data science.\n",
    "\n",
    "## Vectors and Spaces\n",
    "\n",
    "### Definition of Scalers and Vectors\n",
    "\n",
    "- **Scalars**:  These are quantities that have only magnitude (size or amount) but no direction. Think of them like simple numbers that tell you how much of something there is, but not where it's going. A good example is temperature. If it's 30 degrees Celsius, that number tells you how hot it is, but not in which direction the heat is going. Other examples include things like speed, mass, and time.\n",
    "\n",
    "- **Vectors**: On the other hand, vectors are quantities that have both magnitude and direction. They're like arrows pointing somewhere, with the length of the arrow showing how much there is of something and the direction of the arrow showing where it's going. For example, if you're driving a car, your velocity is a vector. It tells you not just how fast you're going (the speed) but also in which direction. Similarly, force is a vector since it's not enough to know how strong the force is; you also need to know in which direction it is applied.\n",
    "\n",
    "So, in short, scalars are just simple numbers indicating how much of something there is, while vectors are like arrows that tell you how much there is and in which direction it's going.\n",
    "\n",
    "- **Scalars vs. Vectors**: Scalars are single numbers, while vectors consist of multiple scalar values.\n",
    "  \n",
    "- **Vector Operations**:\n",
    "    - **Addition**: Adding corresponding elements of two vectors.\n",
    "    - **Subtraction**: Subtracting corresponding elements of two vectors.\n",
    "    - **Scalar Multiplication**: Multiplying each element of a vector by a scalar.\n",
    "\n",
    "\n",
    "### Vector Spaces\n",
    "\n",
    "Vector spaces, \"span\" and \"basis\" are two important concepts that help us understand how vectors work together in a space. Let's break them down in simple terms:\n",
    "- **Span and Basis**:\n",
    "    - **Span**: Set of all possible linear combinations of a set of vectors.The span of a set of vectors is all of the vectors you can make by combining those vectors. When you combine vectors, you're doing two things: scaling them (multiplying them by numbers) and adding them together. The span is like a recipe book that tells you all the different dishes (vectors) you can cook up using a specific set of ingredients (the original vectors). If the span of a set of vectors includes every possible vector in the space, then we say that these vectors span the entire space. For example, in a two-dimensional space (like a flat sheet of paper), if you have two non-parallel arrows (vectors), you can reach any point on the paper by stretching, shrinking, and combining these arrows. So, these two arrows span the two-dimensional space.\n",
    "\n",
    "      \n",
    "    - **Basis**: The minimum set of linearly independent vectors that span a vector space. A basis of a vector space is a set of vectors that are both linearly independent (none of the vectors can be made by combining the others) and span the entire space. You can think of a basis as the most \"efficient\" set of ingredients in your recipe book. With these ingredients (vectors), you can make every possible dish (vector in the space) without any leftovers or waste. In our two-dimensional space example, if you have two arrows that are not pointing in the same or opposite directions, they can serve as a basis. You can reach any point on the paper using combinations of these two arrows, and neither arrow is redundant (you can't create one arrow by using the other).\n",
    "    - In simpler terms, the span tells you what you can reach with your vectors, and the basis is the minimal, non-redundant set of vectors you need to reach everything in the span\n",
    "      \n",
    "- **Linear Independence**: A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5c1f5b-3094-4813-8d3e-3abd85ad8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: [3 7]\n",
      "Subtraction: [ 1 -1]\n",
      "Scalar Multiplication: [4 6]\n"
     ]
    }
   ],
   "source": [
    "## Vectors and Spaces\n",
    "\n",
    "### Python Example: Vector Operations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define two vectors\n",
    "vector1 = np.array([2, 3])\n",
    "vector2 = np.array([1, 4])\n",
    "\n",
    "# Vector addition\n",
    "addition = vector1 + vector2\n",
    "\n",
    "# Vector subtraction\n",
    "subtraction = vector1 - vector2\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar_multiplication = 2 * vector1\n",
    "\n",
    "print(\"Addition:\", addition)\n",
    "print(\"Subtraction:\", subtraction)\n",
    "print(\"Scalar Multiplication:\", scalar_multiplication)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a053f-ae95-4841-bb20-0e97a49c4628",
   "metadata": {},
   "source": [
    "## Matrix Algebra\n",
    "\n",
    "### Matrices and Matrix Operations\n",
    "- **Matrices**: A matrix is a rectangular array of numbers, symbols, or expressions.\n",
    "- **Matrix Operations**:\n",
    "    - **Addition/Subtraction**: Element-wise addition or subtraction.\n",
    "    - **Scalar Multiplication**: Multiplying each element of the matrix by a scalar.\n",
    "    - **Matrix Multiplication**: Combining rows of the first matrix with columns of the second.\n",
    "\n",
    "### Types of Matrices\n",
    "- **Identity Matrix**: A square matrix with 1's along the diagonal and 0's elsewhere.\n",
    "- **Diagonal Matrix**: A matrix where the entries outside the diagonal are all zero.\n",
    "- **Symmetric Matrix**: A matrix that is equal to its transpose.\n",
    "\n",
    "### Matrix Inversion and Determinants\n",
    "- **Inverse of a Matrix**: A matrix that, when multiplied with the original matrix, yields the identity matrix.\n",
    "- **Determinant of a Matrix**: A scalar value that represents the volume scaling factor of the linear transformation described by the matrix.\n",
    "The determinant of a matrix, is a special number that can be calculated from the elements of a square matrix (a matrix with the same number of rows and columns). It has several important interpretations and uses:\n",
    "    \r\n",
    "Volume Scaling Factor: Imagine a shape being transformed in space, like stretching or squishing a cube into a different shape. The determinant tells you how much the volume of that shape changes due to the transformation. A determinant of 1 means the volume stays the same, greater than 1 means the volume increases, and less than 1 means it decreases.    \r\n",
    "\r\n",
    "Matrix Invertibility: The determinant can tell you whether a matrix has an inverse. If the determinant is 0, the matrix cannot be inverted, which in geometric terms means the transformation squashes the shape into a lower-dimensional space (like squishing a 3D shape into a flat 2D shape    ).\r\n",
    "\r\n",
    "Orientation Change: The sign of the determinant (positive or negative) indicates whether the transformation preserves the orientation of the shape. A positive determinant means the orientation is preserved, while a negative determinant means it is reversed (like turning a glove inside out).\r\n",
    "\r\n",
    "In summary, the determinant provides a single value that encapsulates important geometric properties of a matrix, particularly in relation to transformations it represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb242a1c-0fbc-4352-a983-6e8e25b8621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[ 6  8]\n",
      " [10 12]]\n",
      "Matrix Multiplication:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two matrices\n",
    "matrix1 = np.array([[1, 2], [3, 4]])\n",
    "matrix2 = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix addition\n",
    "addition = matrix1 + matrix2\n",
    "\n",
    "# Matrix multiplication\n",
    "multiplication = np.dot(matrix1, matrix2)\n",
    "\n",
    "print(\"Matrix Addition:\\n\", addition)\n",
    "print(\"Matrix Multiplication:\\n\", multiplication)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e98092-e960-4c02-8fb4-c8948b5d1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Matrix (3x3):\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Diagonal Matrix:\n",
      " [[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n",
      "\n",
      "Symmetric Matrix:\n",
      " [[1 2 3]\n",
      " [2 4 5]\n",
      " [3 5 6]]\n",
      "\n",
      "Inverse of a Matrix:\n",
      " [[ 0.6 -0.7]\n",
      " [-0.2  0.4]]\n",
      "\n",
      "Determinant of a Matrix: 10.000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Identity Matrix (3x3)\n",
    "identity_matrix = np.eye(3)\n",
    "print(\"Identity Matrix (3x3):\\n\", identity_matrix)\n",
    "\n",
    "# 2. Diagonal Matrix\n",
    "diagonal_matrix = np.diag([1, 2, 3])\n",
    "print(\"\\nDiagonal Matrix:\\n\", diagonal_matrix)\n",
    "\n",
    "# 3. Symmetric Matrix\n",
    "symmetric_matrix = np.array([[1, 2, 3],\n",
    "                             [2, 4, 5],\n",
    "                             [3, 5, 6]])\n",
    "print(\"\\nSymmetric Matrix:\\n\", symmetric_matrix)\n",
    "\n",
    "# Matrix for inversion and determinant (must be square and non-singular)\n",
    "matrix_for_inversion_and_determinant = np.array([[4, 7],\n",
    "                                                 [2, 6]])\n",
    "\n",
    "# 4. Inverse of a Matrix\n",
    "inverse_matrix = np.linalg.inv(matrix_for_inversion_and_determinant)\n",
    "print(\"\\nInverse of a Matrix:\\n\", inverse_matrix)\n",
    "\n",
    "# 5. Determinant of a Matrix\n",
    "determinant = np.linalg.det(matrix_for_inversion_and_determinant)\n",
    "print(\"\\nDeterminant of a Matrix:\", determinant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcca168-a70b-48cd-be70-e10641ed169e",
   "metadata": {},
   "source": [
    "Calculating the determinant of a matrix is a fundamental operation in linear algebra. The method varies depending on the size of the matrix. Here's a general guide for calculating the determinant:\n",
    "\n",
    "**2x2 Matrix**\n",
    "For a 2x2 matrix, the determinant is calculated as follows:\n",
    "\n",
    "Given a matrix:\n",
    "\n",
    "( a  b\n",
    "\n",
    "  c  d )\n",
    "\n",
    "The determinant is:\n",
    "det = ad−bc\n",
    "\n",
    "**3x3 Matrix**\n",
    "\n",
    "For a 3x3 matrix, the calculation is slightly more complex. The determinant is calculated by expanding along a row or a column (here, I'll use the first row for simplicity):\n",
    "\n",
    "Given a matrix:\n",
    "\n",
    "( a b c\n",
    "\n",
    " d e f\n",
    " \n",
    " g h i )\n",
    "\n",
    "det=a(ei−fh)−b(di−fg)+c(dh−eg)\n",
    "\n",
    "This method involves finding the determinants of three 2x2 matrices and combining them.\n",
    "\n",
    "The determinant of the given matrix \r",
    "\r\n",
    "1\r\n",
    " \r\n",
    "4\r",
    " 3\r\n",
    "\n",
    " \n",
    "\n",
    " \r\n",
    "\r",
    "1\r\n",
    "is \r\n",
    "−\r\n",
    "2.0\r\n",
    "−2.0.\r\n",
    "\r\n",
    "Here's what this determinant tells us:\r\n",
    "\r\n",
    "Volume Change: The absolute value of the determinant (2.0) indicates how much the area (since it's a 2x2 matrix) is scaled when a geometric transformation represented by this matrix is applied. In this case, any area would be scaled by a factor of 2.\r\n",
    "\r\n",
    "Invertibility: Since the determinant is not zero, this matrix is invertible. It means there exists another matrix which, when multiplied with this one, will give the identity matrix.\r\n",
    "\r\n",
    "Orientation: The negative sign of the determinant (-2.0) is significant. It indicates that the transformation would reverse the orientation of the space. For instance, if this matrix were used to transform a shape in 2D space, the shape would be flipped over, like turning a piece of paper over to its other side.\r\n",
    "\r\n",
    "So, the determinant gives us valuable information about how the matrix transforms space, including scaling, invertibility, and orientation changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccde68d-18e6-4ed4-84c0-e33fab3ee7c5",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "### Introduction to Eigenvalues and Eigenvectors\n",
    "- **Definition**: \n",
    "Eigenvalues and eigenvectors are concepts from linear algebra that have broad applications in fields like engineering, physics, and data analysis. Let's break them down into layman's terms:\n",
    "\n",
    "- **Eigenvectors**:  Non-zero vectors that change at most by a scalar factor when that linear transformation is applied.Imagine you have a transformation machine that changes the shape or direction of objects. You put in a vector (think of it as an arrow pointing in a certain direction), and the machine transforms it. Now, most vectors will come out of this machine pointing in a different direction. However, there might be some special vectors (eigenvectors) that come out still pointing in their original direction, though their length might have changed. These special vectors are called eigenvectors of the transformation.\n",
    "\n",
    "- **Eigenvalues**:Scalars that measure the factor by which the eigenvector is scaled during a linear transformation.The eigenvalue is a number associated with each eigenvector. It tells you how much the length of the eigenvector changes when it goes through the transformation. If the eigenvalue is 2, for example, the eigenvector becomes twice as long. If it's 0.5, the eigenvector becomes half as long. If the eigenvalue is 1, the eigenvector doesn't change in length at all.\n",
    "\n",
    "In simpler terms, in the world of transformations, eigenvectors are the vectors that manage to keep their direction, and eigenvalues tell us how their length is altered. They are important because they reveal the fundamental properties of the transformation, such as which directions are stretched or shrunk and by how much.\n",
    "\n",
    "### Calculating Eigenvalues and Eigenvectors\n",
    "- **Characteristic Polynomial**: A polynomial which is derived from the determinant of a matrix and is used to find eigenvalues.\n",
    "- **Applications in Dimensionality Reduction (PCA)**:\n",
    "    - Principal Component Analysis (PCA) uses eigenvalues and eigenvectors to reduce the dimensionality of data sets, improving the interpretability while minimizing information loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e136d-a228-43e1-bcdb-12ac9b0d992d",
   "metadata": {},
   "source": [
    "### Calculating Eigenvalues\n",
    "\n",
    "* Characteristic Polynomial: For a matrix A, you start by finding the characteristic polynomial, which is the determinant of A−λI, where λ is a scalar (the eigenvalue we're trying to find) and I is the identity matrix of the same size as A. This gives you a polynomial in terms of λ.\n",
    "\n",
    "* Solve for λ: The eigenvalues are the roots of this polynomial. This means you solve the equation \n",
    "\n",
    "det(A−λI)=0 for λ.\n",
    "\n",
    "### Calculating Eigenvectors\n",
    "Once you have the eigenvalues, you can find the corresponding eigenvectors:\n",
    "\n",
    "* Set Up the Equation: For each eigenvalue λ, set up the equation (A−λI)  =0, where v is the eigenvector corresponding to λ.\n",
    "\n",
    "* Solve for v: Solve this system of linear equations to find the eigenvector v. This usually involves Gaussian elimination or other methods of solving linear systems.\n",
    "\n",
    "#### Example\n",
    "Let's illustrate this with a simple 2x2 matrix example. Consider the matrix \n",
    "\n",
    "(4 2\n",
    "\n",
    "3 1)\n",
    "\n",
    "* Characteristic Polynomial:\n",
    "\n",
    "det(A−λI)=det(4−λ  2\n",
    "              3   1−λ )=(4−λ)(1−λ)−2×3\n",
    "Simplify this to get a quadratic equation in terms of λ.\n",
    "\n",
    "* Solve for λ:\n",
    "Find the roots of this quadratic equation. These roots are your eigenvalues.\n",
    "\n",
    "* Solve for v:\n",
    "For each eigenvalue λ, plug it back into (A−λI) v =0 and solve for v.\n",
    "\n",
    "This process can be quite involved, especially for matrices larger than 2x2, as finding roots of high-degree polynomials can be complex. For practical purposes, computational tools like NumPy in Python are typically used to efficiently compute eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe365948-6604-4c34-9b1e-2e3011e34794",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ajXb3N6QEqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1733ec1-0964-42bb-b72d-ac0abe8414fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 0.89442719 -0.70710678]\n",
      " [ 0.4472136   0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a matrix\n",
    "matrix = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac3985-04c0-4032-8bf5-ba36d3d99be2",
   "metadata": {},
   "source": [
    "\n",
    "## Singular Value Decomposition\n",
    "\n",
    "SVD is a technique that decomposes a matrix into three distinct components. Each component captures a different aspect of the original matrix's structure and information. The original matrix can be thought of as containing mixed information, and SVD separates this information into more interpretable and manageable parts.\n",
    "\n",
    "- **Concept and Mathematical Formulation**: \n",
    "    - Decomposition of a matrix into three other matrices. It provides a way of diagonalizing a matrix.\n",
    "    - Equation: `A = UΣV*` where A is the original matrix, U and V are orthogonal matrices, and Σ is a diagonal matrix of singular values.\n",
    " \n",
    "### Understanding SVD\n",
    "Singular Value Decomposition (SVD) is a powerful mathematical concept, often used in data analysis and signal processing. To explain it in layman's terms, let's use a metaphor:\n",
    "\n",
    "Imagine you have a mixed fruit smoothie, and you want to understand its ingredients and their proportions. SVD is like a magical process that separates this smoothie back into its individual components: different fruits and their respective quantities.\n",
    "\n",
    "In mathematical terms, SVD breaks down a complex matrix (the smoothie) into simpler, meaningful components. Here's what happens:\n",
    "\n",
    "1. Matrix: Think of a matrix as a grid of numbers. In real-world terms, this could represent anything from image data to user ratings on a website.\n",
    "\n",
    "2. Decomposition: SVD decomposes this matrix into three special matrices:\n",
    "\n",
    "    * The first matrix describes the original row items (like different fruits) in terms of underlying features.\n",
    "    * The second matrix is a diagonal matrix, and it contains singular values. These values tell you the 'strength' or 'importance' of each feature. It’s like telling you how much of each fruit is present in your smoothie.\n",
    "    * The third matrix describes the original column items in terms of the same underlying features.\n",
    "3. Application: This decomposition is extremely useful. For example, in image processing, it helps in compressing the image by keeping only the most important features. In recommendation systems, it helps identify underlying patterns in user behavior.\n",
    "\n",
    "So, SVD is a way to extract and highlight the most important features from complex data sets, just like identifying individual ingredients and their proportions in a mixed smoothie.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9dc7cf-6013-44de-baa2-93b9b0ce3670",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=DG7YTlGnCEo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10010059-8a64-48e5-a152-d59487e8b71e",
   "metadata": {},
   "source": [
    "### Explanation of SVD Matrices\n",
    "\n",
    "The three matrices obtained from Singular Value Decomposition (SVD) – \\( U \\), Σ , and \\( V^T \\) – each have a specific interpretation and role in the decomposition process:\n",
    "\n",
    "#### 1. Matrix U  (Left Singular Vectors)\n",
    "- Represents the 'row space' of the original matrix.\n",
    "- Each column in U is a vector that captures a specific pattern or direction present in the rows of the original matrix.\n",
    "- These vectors are orthonormal, meaning they are perpendicular to each other and each has unit length.\n",
    "- Think of U as a set of directions that, in combination, can reconstruct the row patterns of the original matrix.\n",
    "- The columns of matrix \\( U \\) are the left singular vectors of \\( A \\).\n",
    "- These vectors are orthonormal, meaning they are orthogonal (at right angles to each other) and normalized (each has unit length).\n",
    "- They represent different dimensions or directions in which the data in matrix \\( A \\) varies, capturing specific patterns or features in the rows of \\( A \\).\n",
    "\n",
    "#### 2. Matrix Σ (Singular Values)\n",
    "- \\( \\Sigma \\) is a diagonal matrix containing the singular values of \\( A \\).\n",
    "- Singular values are non-negative and usually arranged in descending order.\n",
    "- They measure the importance or strength of each corresponding singular vector, indicating how much of the data's variability or information is captured.\n",
    "- The magnitude of these singular values often quickly drops, allowing for dimensionality reduction by disregarding the lower singular values and their vectors.\n",
    "- Large singular values correspond to more significant patterns or directions in the data, while smaller values correspond to less significant ones.\n",
    "- Σ essentially scales the vectors in U and V^T   to reconstruct the original matrix's magnitude and importance of features.\n",
    "\n",
    "#### 3. Matrix \\( V^T \\) (Right Singular Vectors)\n",
    "- Represents the 'column space' of the original matrix.\n",
    "- The rows of \\( V^T \\) (or columns of \\( V \\)) are the right singular vectors of \\( A \\).\n",
    "- These vectors are also orthonormal.\n",
    "- They represent different dimensions or directions in the columns of \\( A \\), capturing specific patterns or features in how the columns relate to each other.\n",
    "\n",
    "##### Putting It All Together\n",
    "- The product ( U Σ V^T ) reconstructs the original matrix \\( A \\), highlighting the data's composition from patterns in \\( U \\) and \\( V \\), weighted by Σ.\n",
    "- In data analysis, \\( U \\) and \\( V \\) reveal the underlying structure of the data, while (Σ) quantifies the significance of each structure.\n",
    "\n",
    "##### Application Example: Image Processing\n",
    "- In image compression, \\( U \\) and \\( V \\) might capture visual features like edges or textures, with Σ indicating the prominence of each feature. By retaining only the significant features, a compressed approximation of the image can be created.\n",
    "\n",
    "In summary, SVD breaks down a matrix into components that reveal its intrinsic geometric and algebraic properties, separating the original data into orthogonal components of variation and quantifying their importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ce967-396f-457c-a712-8aad5048c45d",
   "metadata": {},
   "source": [
    "### How to Calculate SVD Using Formulas\n",
    "\n",
    "Singular Value Decomposition (SVD) is a mathematical method used to decompose a matrix into three distinct matrices. The process involves several steps and is fundamental in linear algebra. Below is an outline of how to calculate SVD using formulas:\n",
    "\n",
    "#### Step 1: Define the Matrix\n",
    "First, define the matrix `A` that you want to decompose. `A` can be of any size, not necessarily square.\n",
    "\n",
    "#### Step 2: Compute the Matrices `A^T A` and `AA^T`\n",
    "- Calculate the transpose of `A`, denoted as `A^T`.\n",
    "- Compute the matrices `A^T A` and `AA^T`. These are square matrices.\n",
    "\n",
    "#### Step 3: Find the Eigenvalues\n",
    "- Calculate the eigenvalues of `A^T A` and `AA^T`.\n",
    "- These eigenvalues will be the squared singular values of `A`. They are always non-negative.\n",
    "\n",
    "#### Step 4: Calculate the Eigenvectors\n",
    "- Compute the eigenvectors of `A^T A` and `AA^T`.\n",
    "- The eigenvectors of `A^T A` form the columns of the matrix `V`.\n",
    "- The eigenvectors of `AA^T` form the columns of the matrix `U`.\n",
    "\n",
    "#### Step 5: Form the Singular Value Matrix\n",
    "- Take the square roots of the eigenvalues found in Step 3.\n",
    "- These are the singular values of `A`.\n",
    "- Construct a diagonal matrix `Σ` with these singular values.\n",
    "\n",
    "#### Step 6: Assemble the Decomposition\n",
    "- The final SVD of `A` is given by `UΣV^T`, where `V^T` is the transpose of `V`.\n",
    "\n",
    "#### Notes:\n",
    "- For a real matrix, `U` and `V` are orthogonal matrices, and `Σ` is a diagonal matrix.\n",
    "- The number of non-zero singular values is equal to the rank of matrix `A`.\n",
    "- The process can be computationally intensive for large matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5edac0-1c7f-4b64-bbd4-6fedbacab61d",
   "metadata": {},
   "source": [
    "### Applications of SVD in Data Science\n",
    "- **Data Compression**: Reducing the size of data by eliminating redundant features.\n",
    "- **Noise Reduction**: Improving the quality of data by removing noise.\n",
    "- **Recommender Systems** Imagine a service like Netflix, which has a vast number of movies (items) and millions of users. Each user has different preferences and has rated only a subset of movies. The challenge for Netflix is to recommend movies that a user is likely to enjoy but hasn't watched yet.\n",
    "\n",
    "#### **Recommender Systems**\n",
    "#### How SVD Helps:\n",
    "1. Matrix Representation: Netflix can represent the user ratings as a matrix where each row represents a user, and each column represents a movie. The values in the matrix are the ratings given by users to movies. Since not all users have rated all movies, this matrix has many missing values.\n",
    "\n",
    "2. Applying SVD: SVD is used to factorize this matrix into three smaller matrices. The key idea here is dimensionality reduction. SVD helps to identify latent factors that represent underlying patterns in the ratings. These factors might correspond to genres, movie characteristics (like action, drama, romance), user preferences, etc.\n",
    "\n",
    "3. Prediction of Missing Ratings: Once the matrix is factorized, Netflix can use the decomposed matrices to predict the ratings a user might give to a movie they haven't seen yet. This is done by recombining the factors in a way that approximates the original matrix but with filled-in missing values.\n",
    "\n",
    "4. Making Recommendations: Based on these predicted ratings, Netflix can recommend movies to a user that they are likely to rate highly.\n",
    "\n",
    "#### Advantages:\n",
    "* Dealing with Sparse Data: SVD is particularly useful when dealing with sparse data (like a user-item matrix with many missing ratings).\n",
    "* Uncovering Hidden Patterns: It can uncover hidden patterns in the data that are not immediately obvious, improving the quality of recommendations.\n",
    "\n",
    "**Example in Python:**\n",
    "In Python, libraries like scikit-learn or surprise can be used to implement SVD for recommender systems. The process involves building a matrix of user-item ratings, applying SVD to this matrix, and then using the factorized matrices to predict missing ratings and generate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7ba67e-5a79-48b3-937f-771de6e13097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "[9.52551809 0.51430058]\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrix\n",
    "matrix = np.array([[1, 2],\n",
    "                   [3, 4],\n",
    "                   [5, 6]])\n",
    "\n",
    "# Performing Singular Value Decomposition\n",
    "U, singular_values, VT = np.linalg.svd(matrix)\n",
    "print(U)\n",
    "print(singular_values)\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb27ea-1c4a-4de7-966d-54ef03af474a",
   "metadata": {},
   "source": [
    "## Applications in Data Science\n",
    "\n",
    "### Linear Algebra in Machine Learning\n",
    "- **Feature Spaces**: Using vector spaces to represent features in machine learning models.\n",
    "- **Linear Regression, PCA, and Other Algorithms**: \n",
    "    - Linear regression involves matrix operations for predicting outcomes.\n",
    "    - PCA for dimensionality reduction.\n",
    "    - Other algorithms where linear algebra is pivotal.\n",
    "\n",
    "### Network Analysis\n",
    "- **Adjacency Matrices**: Representing graphs and networks using matrices.\n",
    "- **Graph Theory Concepts**: Understanding the structure and properties of networks.\n",
    "\n",
    "## References\n",
    "- [Linear Algebra and Its Applications, David C. Lay]\n",
    "- [Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares, Stephen Boyd and Lieven Vandenberghe]\n",
    "- [Online resources and courses for further study.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ded4fc-acdd-486e-92b8-8987db0b1749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
